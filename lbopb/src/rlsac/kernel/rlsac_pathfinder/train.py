# SPDX-License-Identifier: GPL-3.0-only
# Copyright (C) 2025 GaoZheng
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, version 3 only.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
#
# --- è‘—ä½œæƒç‹¬ç«‹æ€§å£°æ˜ (Copyright Independence Declaration) ---
# æœ¬æ–‡ä»¶ï¼ˆâ€œè½½è·â€ï¼‰æ˜¯ä½œè€… (GaoZheng) çš„åŸåˆ›è‘—ä½œç‰©ï¼Œå…¶çŸ¥è¯†äº§æƒ
# ç‹¬ç«‹äºå…¶è¿è¡Œå¹³å° GROMACSï¼ˆâ€œå®¿ä¸»â€ï¼‰ã€‚
# æœ¬æ–‡ä»¶çš„æˆæƒéµå¾ªä¸Šè¿° SPDX æ ‡è¯†ï¼Œä¸å—â€œå®¿ä¸»â€è®¸å¯è¯çš„ç®¡è¾–ã€‚
# è¯¦æƒ…å‚è§é¡¹ç›®æ–‡æ¡£ "my_docs/project_docs/1762636780_ğŸš©ğŸš©gromacs-2024.1_developeré¡¹ç›®çš„è‘—ä½œæƒè®¾è®¡ç­–ç•¥ï¼šâ€œå®¿ä¸»-è½½è·â€ä¸â€œåŒè½¨åˆ¶â€å¤åˆæ¶æ„.md"ã€‚
# ------------------------------------------------------------------

from __future__ import annotations

import json
import time as _pytime
from pathlib import Path
import os
import re
import re
from typing import Any, Dict, List

import torch
import torch.nn.functional as F

try:
    from .env_domain import DomainPathfinderEnv, Goal
    from .domain import get_domain_spec
except ImportError:
    # å…¼å®¹ç›´æ¥è„šæœ¬æ‰§è¡Œï¼šå°†ä»“åº“æ ¹åŠ å…¥ sys.path åç”¨ç»å¯¹å¯¼å…¥
    from pathlib import Path as _Path
    import sys as _sys

    _sys.path.insert(0, str(_Path(__file__).resolve().parents[5]))
    from lbopb.src.rlsac.kernel.rlsac_pathfinder.env_domain import DomainPathfinderEnv, Goal
    from lbopb.src.rlsac.kernel.rlsac_pathfinder.domain import get_domain_spec
try:
    from .env_domain import DomainPathfinderEnv, Goal
    from .domain import get_domain_spec
    from .sampler import sample_random_package
    from .oracle import AxiomOracle, default_init_state, apply_sequence
    from .scorer import PackageScorer, train_scorer
except ImportError:
    # å…¼å®¹ç›´æ¥è„šæœ¬æ‰§è¡Œï¼šå°†ä»“åº“æ ¹åŠ å…¥ sys.path åç”¨ç»å¯¹å¯¼å…¥
    from pathlib import Path as _Path
    import sys as _sys

    _sys.path.insert(0, str(_Path(__file__).resolve().parents[5]))
    from lbopb.src.rlsac.kernel.rlsac_pathfinder.env_domain import DomainPathfinderEnv, Goal
    from lbopb.src.rlsac.kernel.rlsac_pathfinder.domain import get_domain_spec
    from lbopb.src.rlsac.kernel.rlsac_pathfinder.sampler import sample_random_package
    from lbopb.src.rlsac.kernel.rlsac_pathfinder.oracle import AxiomOracle, default_init_state, apply_sequence
    from lbopb.src.rlsac.kernel.rlsac_pathfinder.scorer import PackageScorer, train_scorer
from lbopb.src.rlsac.application.rlsac_nsclc.utils import select_device_from_config

try:
    from lbopb.src.rlsac.application.rlsac_nsclc.models import DiscretePolicy  # type: ignore
except Exception:
    DiscretePolicy = None  # type: ignore


def _load_config(cfg_path: Path) -> Dict[str, Any]:
    return json.loads(cfg_path.read_text(encoding="utf-8"))


def _resolve_domain(cfg: Dict[str, Any], override: str | None = None) -> str:
    """è§£æåŸŸæ ‡è¯†ï¼š
    - è‹¥æä¾› overrideï¼ˆå­—ç¬¦ä¸²ï¼‰ï¼Œä¼˜å…ˆè¿”å›å…¶å°å†™ï¼›
    - è‹¥ cfg["domain"] ä¸ºå­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›å…¶å°å†™ï¼›
    - è‹¥ cfg["domain"] ä¸ºæ•°å­—ï¼Œåˆ™åœ¨ cfg["domain_choose"] ä¸­æŒ‰å€¼åæŸ¥å¯¹åº” keyï¼›
    - å…œåº•è¿”å› "pem"ã€‚
    """
    if override is not None:
        try:
            return str(override).strip().lower()
        except Exception:
            pass
    dom = cfg.get("domain", None)
    if isinstance(dom, str):
        return dom.strip().lower()
    try:
        dom_num = int(dom)  # type: ignore[arg-type]
        mapping = cfg.get("domain_choose", {}) or {}
        if isinstance(mapping, dict):
            for k, v in mapping.items():
                try:
                    if int(v) == dom_num:
                        return str(k).strip().lower()
                except Exception:
                    continue
    except Exception:
        pass
    return "pem"


def build_env_from_config(cfg_path: Path, domain_override: str | None = None) -> DomainPathfinderEnv:
    cfg = _load_config(cfg_path)
    domain = _resolve_domain(cfg, domain_override)
    # è¯»å–åŸŸå·®å¼‚é…ç½®ï¼ˆconfig.dict.jsonï¼‰
    mod_dir = Path(__file__).resolve().parent
    dict_path = mod_dir / "config.dict.json"
    dict_cfg: Dict[str, Any] = {}
    try:
        if dict_path.exists():
            dict_cfg = json.loads(dict_path.read_text(encoding="utf-8"))
    except Exception:
        dict_cfg = {}
    dom_defaults = (dict_cfg.get("domains", {}) or {}).get(domain, {}) if isinstance(dict_cfg, dict) else {}
    spec = get_domain_spec(domain)
    # ä¼˜å…ˆä½¿ç”¨ config.json ä¸­çš„è¦†ç›–é¡¹ï¼Œå¦åˆ™é‡‡ç”¨ config.dict.json çš„åŸŸé»˜è®¤
    s0_cfg = cfg.get("initial_state",
                     dom_defaults.get("initial_state", {"b": 3.0, "n_comp": 3, "perim": 5.0, "fidelity": 0.4}))
    st_cfg = cfg.get("target_state",
                     dom_defaults.get("target_state", {"b": 0.5, "n_comp": 1, "perim": 2.0, "fidelity": 0.9}))
    tol_cfg = cfg.get("tolerance", dom_defaults.get("tolerance", {"b": 0.1, "n": 0.5, "p": 0.2, "f": 0.05}))
    init_state = spec.state_cls(
        b=float(s0_cfg.get("b", 3.0)),
        n_comp=int(s0_cfg.get("n_comp", 3)),
        perim=float(s0_cfg.get("perim", 5.0)),
        fidelity=float(s0_cfg.get("fidelity", 0.4)),
    )
    goal = Goal(
        target=spec.state_cls(
            b=float(st_cfg.get("b", 0.5)),
            n_comp=int(st_cfg.get("n_comp", 1)),
            perim=float(st_cfg.get("perim", 2.0)),
            fidelity=float(st_cfg.get("fidelity", 0.9)),
        ),
        tol_b=float(tol_cfg.get("b", 0.1)),
        tol_n=float(tol_cfg.get("n", 0.5)),
        tol_p=float(tol_cfg.get("p", 0.2)),
        tol_f=float(tol_cfg.get("f", 0.05)),
    )
    max_steps = int(cfg.get("episode_max_steps", 64))
    improve_w = float(cfg.get("improve_weight", 1.0))
    step_penalty = float(cfg.get("step_penalty", 0.01))
    include_identity = bool(cfg.get("include_identity", False))
    env = DomainPathfinderEnv(
        spec,
        init_state=init_state,
        goal=goal,
        max_steps=max_steps,
        improve_weight=improve_w,
        step_penalty=step_penalty,
        include_identity=include_identity,
    )
    return env


def train(config_path: str | Path | None = None, domain_override: str | None = None) -> Path:
    mod_dir = Path(__file__).resolve().parent
    cfg_path = Path(config_path) if config_path else (mod_dir / "config.json")
    cfg = _load_config(cfg_path)
    device = select_device_from_config(cfg_path)

    debug = bool(cfg.get("debug", False))
    log_every_step = bool(cfg.get("log_every_step", True))
    log_to_file = bool(cfg.get("log_to_file", True))

    # Debug æ¨¡å¼ä¸‹ï¼Œæ‰“å¼€ Gemini å…¨ç¨‹è°ƒè¯•ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’ç»™ HTTP å®¢æˆ·ç«¯ï¼‰
    if debug:
        try:
            os.environ.setdefault("LBOPB_GEMINI_DEBUG", "1")
        except Exception:
            pass
    # ä»é…ç½®ä¼ é€’ LLM è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ç»™ HTTP å®¢æˆ·ç«¯
    try:
        _llm_to = cfg.get("llm_timeout_sec", None)
        if _llm_to is not None:
            os.environ["LBOPB_GEMINI_TIMEOUT_SEC"] = str(_llm_to)
    except Exception:
        pass
    # ä»é…ç½®è§£æ Gemini æ¨¡å‹é€‰æ‹©ï¼ˆåç§°æˆ–ç¼–å·ï¼‰ï¼Œå¹¶ä¸‹å‘ç»™ç¯å¢ƒå˜é‡
    gemini_model: str | None = None
    try:
        gm = cfg.get("GEMINI_MODEL", None)
        name = None
        if isinstance(gm, str):
            name = gm.strip()
        elif isinstance(gm, int):
            mp = cfg.get("gemini_model_choose", {}) or cfg.get("GEMINI_MODEL_CHOOSE", {}) or {}
            if isinstance(mp, dict):
                for k, v in mp.items():
                    try:
                        if int(v) == gm:
                            name = str(k)
                            break
                    except Exception:
                        continue
        if name:
            os.environ["LBOPB_GEMINI_MODEL"] = name
            # å¼ºåˆ¶è¦†ç›–ä»¥é¿å…å¤–éƒ¨é»˜è®¤å€¼å¹²æ‰°
            os.environ["GEMINI_MODEL"] = name
            gemini_model = name
    except Exception:
        pass

    # LLM è¯·æ±‚èŠ‚æµï¼ˆç§’ï¼‰
    llm_req_interval = float(cfg.get("llm_request_interval_sec", 0.0))
    last_llm_time: float = 0.0

    class RunLogger:
        def __init__(self, path: Path, append: bool = False):
            self.path = path
            self.f = open(path, 'a' if append else 'w', encoding='utf-8', newline='')

        def write_line(self, text: str) -> None:
            try:
                self.f.write(text + "\n")
                self.f.flush()
            except Exception:
                pass

        def close(self) -> None:
            try:
                self.f.flush()
                self.f.close()
            except Exception:
                pass

    def dbg(msg: str) -> None:
        if debug:
            print(f"[DEBUG] {msg}")
            if logger:
                logger.write_line(f"[DEBUG] {msg}")

    def step_log(msg: str) -> None:
        if log_every_step or debug:
            print(msg)
            if logger:
                logger.write_line(msg)

    # æ§åˆ¶å°é¢œè‰²ä¸æ—¥å¿—è¾…åŠ©ï¼ˆæ§åˆ¶å°å½©è‰²ï¼Œæ—¥å¿—å»è‰²ï¼‰
    ANSI_RESET = "\x1b[0m"
    ANSI_RED = "\x1b[31;1m"
    ANSI_GREEN = "\x1b[32;1m"
    ANSI_YELLOW = "\x1b[33;1m"
    ANSI_CYAN = "\x1b[36;1m"
    ANSI_MAGENTA = "\x1b[35;1m"

    _ansi_re = re.compile(r"\x1b\[[0-9;]*m")

    def _strip_ansi(s: str) -> str:
        try:
            return _ansi_re.sub("", s)
        except Exception:
            return s

    def logc(text: str, color: str | None = None) -> None:
        s = f"{color}{text}{ANSI_RESET}" if (color and debug) else text
        print(s)
        if logger:
            logger.write_line(_strip_ansi(text))

    # ç¯å¢ƒ
    # ç¡®å®šæœ¬æ¬¡è®­ç»ƒçš„åŸŸ
    this_domain = _resolve_domain(cfg, domain_override)

    # é‡‡æ ·-ç›‘ç£ï¼šéšæœºç”Ÿæˆç®—å­åŒ… â†’ å…¬ç†ç³»ç»Ÿæ‰“åˆ†(0/1) â†’ è®­ç»ƒæ‰“åˆ†ç½‘ç»œ
    min_len = int(cfg.get("min_len", 1))
    max_len = int(cfg.get("max_len", 4))
    no_dup = bool(cfg.get("sampler_no_consecutive_dup", True))
    n_samples = int(cfg.get("samples", 2000))
    epochs = int(cfg.get("epochs", 20))
    batch_size = int(cfg.get("batch_size", 64))
    topk = int(cfg.get("topk", 50))
    cost_lambda = float(cfg.get("cost_lambda", 0.2))
    oracle = AxiomOracle(cost_lambda=cost_lambda, use_llm=bool(cfg.get("use_llm_oracle", False)))

    spec = get_domain_spec(this_domain)
    op_names = []
    # ç¡®å®šç‰¹å¾ç»´åº¦ï¼šæ¯ä¸ªåŸºæœ¬ç®—å­ä¸€ä¸ªè®¡æ•° + åºåˆ—é•¿åº¦ + Î”risk + cost
    for cls in spec.op_classes:
        try:
            nm = cls().name
        except Exception:
            nm = cls.__name__
        op_names.append(str(nm))
    feat_dim = len(op_names) + 3

    # å‡†å¤‡è¿è¡Œç›®å½•ï¼ˆä¾¿äºè°ƒè¯•äº§ç‰©å†™å…¥ï¼‰
    repo_root = Path(__file__).resolve().parents[5]
    out_root = repo_root / "out"
    base_out = out_root / Path(cfg.get("output_dir", "out_pathfinder"))
    base_out.mkdir(parents=True, exist_ok=True)
    _stamp = str(int(_pytime.time()))
    explore_only = bool(cfg.get("explore_only", True))
    run_prefix = "dataset_" if explore_only else "train_"
    run_name = run_prefix + _stamp + (f"_{this_domain}" if this_domain else "")
    run_dir = base_out / run_name
    run_dir.mkdir(parents=True, exist_ok=True)
    log_path = run_dir / "train.log"
    if log_to_file:
        logger = RunLogger(log_path, append=False)
        logger.write_line(f"# TRAIN START {_pytime.strftime('%Y-%m-%d %H:%M:%S', _pytime.localtime())}")
        logger.write_line(f"device={device}")
        logger.write_line(f"config={json.dumps(cfg, ensure_ascii=False)}")

    # è®­ç»ƒæ ·æœ¬å°†æŒ‰çº³å…¥è§„åˆ™åŠ¨æ€æ”¶é›†ï¼Œä¸å†é¢„å…ˆåˆ†é…å¼ é‡
    debug_dump = bool(cfg.get("debug_dump", True))
    dataset_dump: list[dict[str, Any]] = [] if debug_dump else []
    init_state = default_init_state(this_domain)

    def evaluate_with_details(domain: str, seq: List[str], s0: Any | None = None) -> dict[str, Any]:
        import importlib
        s0 = s0 if s0 is not None else default_init_state(domain)
        # syntax check
        syntax = {"errors": [], "warnings": [], "valid": True}
        try:
            mod = importlib.import_module(f"lbopb.src.{domain}.syntax_checker")
            func = getattr(mod, "check_sequence", None)
            if callable(func):
                res = func(list(seq), init_state=s0)
                syntax = {
                    "errors": list(res.get("errors", []) or []),
                    "warnings": list(res.get("warnings", []) or []),
                    "valid": bool(res.get("valid", True)),
                }
        except Exception:
            pass
        # heuristicsï¼ˆä»…ç”¨äºè°ƒè¯•è§‚æµ‹ï¼Œä¸ä½œä¸ºè®­ç»ƒæ ‡ç­¾å†³ç­–ä¾æ®ï¼‰
        _, dr, c = apply_sequence(domain, s0, seq)
        heur_ok = bool((dr - cost_lambda * c) > 0.0)
        # LLM only when warnings present and enabled
        llm_used = False
        llm_attempted = False
        llm_raw: str | None = None
        llm_prompt: str | None = None
        llm_status: str = "skipped_no_warn"  # used | skipped_no_warn | skipped_use_llm_false | skipped_errors | skipped_exception
        # è®­ç»ƒæ ·æœ¬çº³å…¥è§„åˆ™ï¼š
        # - syntax æœ‰ errors: æ˜ç¡®é”™è¯¯ï¼Œçº³å…¥è®­ç»ƒï¼ˆè´Ÿæ ·æœ¬ï¼‰
        # - syntax æ— é”™è¯¯ä½†æœ‰ warnings:
        #     - è‹¥å¯ç”¨ LLM ä¸”è°ƒç”¨æˆåŠŸè¿”å› '1'/'0'ï¼šæŒ‰è¿”å›çº³å…¥è®­ç»ƒï¼›
        #     - å¦åˆ™ï¼ˆæœªå¯ç”¨æˆ–è°ƒç”¨å¤±è´¥ï¼‰ï¼šä¸çº³å…¥è®­ç»ƒï¼›
        # - æ— é”™è¯¯ä¸”æ— è­¦å‘Šï¼šæ˜ç¡®æ­£ç¡®ï¼Œçº³å…¥è®­ç»ƒï¼ˆæ­£æ ·æœ¬ï¼‰ã€‚
        train_include = False
        train_reason = ""
        # ç¡®ä¿ä»»ä½•è·¯å¾„ä¸‹éƒ½æœ‰ label åˆå€¼ï¼Œé¿å…æœªç»‘å®šé”™è¯¯
        label = 0
        if syntax["errors"]:
            label = 0
            train_include = True
            train_reason = "syntax_error"
            llm_status = "skipped_errors"
        else:
            warns_present = bool(syntax.get("warnings"))
            llm_force = bool(cfg.get("llm_force_dual_validation", False))
            if (warns_present or llm_force) and bool(cfg.get("use_llm_oracle", False)):
                try:
                    from lbopb.src.rlsac.kernel.common.llm_oracle import call_llm, build_pathfinder_prompt
                    # å¯é€‰: å°†ç®—å­å‚æ•°åŒ–ä¸å–å€¼ä¸€å¹¶æäº¤ç»™ LLM
                    ops_det = None
                    extra_meta = None
                    try:
                        if bool(cfg.get("llm_include_params", False)):
                            from lbopb.src.rlsac.kernel.rlsac_pathfinder.op_space_utils import load_op_space, param_grid_of, params_from_grid  # type: ignore
                            # æ¨å¯¼è¯¥åŸŸçš„ç©ºé—´å®šä¹‰æ–‡ä»¶ï¼ˆv1ï¼‰
                            _mod_dir = Path(__file__).resolve().parent
                            _space_ref = _mod_dir / "operator_spaces" / f"{domain}_op_space.v1.json"
                            if _space_ref.exists():
                                _space = load_op_space(str(_space_ref))
                                # ä¸ºæ¯ä¸ªç®—å­é€‰æ‹©ä¸€ä¸ªâ€œé»˜è®¤â€ç´¢å¼•ï¼ˆç½‘æ ¼ä¸­ä½æ•°ï¼‰ï¼ŒåæŸ¥ params
                                steps = []
                                for nm in list(seq):
                                    try:
                                        names, grids = param_grid_of(_space, nm)
                                    except Exception:
                                        # æ— å‚æ•°å®šä¹‰åˆ™è·³è¿‡å‚æ•°åŒ–
                                        steps.append({"name": nm})
                                        continue
                                    gi: list[int] = []
                                    for g in grids:
                                        L = len(g)
                                        gi.append(max(0, (L - 1) // 2))
                                    prs = params_from_grid(_space, nm, gi)
                                    steps.append({"name": nm, "grid_index": gi, "params": prs})
                                ops_det = steps
                                extra_meta = {
                                    "op_space_id": _space.get("space_id", None),
                                    "op_space_ref": str(_space_ref).replace("\\", "/"),
                                }
                    except Exception:
                        ops_det = None
                        extra_meta = None
                    llm_prompt = build_pathfinder_prompt(domain, list(seq), ops_det, extra_meta)
                    if debug:
                        # åˆ†å‰²çº¿ä¸è¯·æ±‚å‰æ‰“å°
                        logc("========== LLM REQUEST BEGIN ==========", ANSI_CYAN)
                        # ä¼˜å…ˆæ‰“å°åºåˆ—ä¸å‚æ•°æ®µï¼Œä¾¿äºå¿«é€Ÿå®šä½å…³é”®ä¿¡æ¯
                        logc(f"[SEQ] {seq}", ANSI_YELLOW)
                        if ops_det:
                            try:
                                import json as _json
                                _ops_preview = _json.dumps(ops_det, ensure_ascii=False)[:240]
                                logc(f"[PARAMS] {_ops_preview}", ANSI_YELLOW)
                            except Exception:
                                pass
                        # å…¶åå†æ‰“å° LLM å¯åŠ¨ä¿¡æ¯ä¸æç¤ºè¯é¢„è§ˆ
                        _msg0 = (
                            f"[LLM] start: provider=gemini domain={domain} seq_len={len(seq)} "
                            f"warnings={len(syntax.get('warnings', []))} heur_ok={heur_ok}"
                        )
                        logc(_msg0, ANSI_CYAN)
                        try:
                            _pv = str(llm_prompt)
                            # æ‰“å°é¢„è§ˆæ—¶å‹æˆå•è¡Œï¼šæ¢è¡Œâ†’\nï¼Œåˆ¶è¡¨â†’ç©ºæ ¼ï¼Œè¿ç»­ç©ºç™½å‹ç¼©
                            _pv_oneline = _pv.replace("\r\n", "\n").replace("\r", "\n").replace("\t", " ").replace("\n", "\\n")
                            # é€‚å½“æˆªæ–­ï¼Œé¿å…åˆ·å±
                            _pv2 = _pv_oneline[:240] + ("..." if len(_pv_oneline) > 240 else "")
                            _msg1 = f"[LLM] prompt preview: {_pv2}"
                            logc(_msg1, ANSI_CYAN)
                        except Exception:
                            pass
                    # èŠ‚æµï¼šè‹¥è®¾ç½®äº†è¯·æ±‚é—´éš”ï¼Œåˆ™åœ¨ä¸¤æ¬¡è¯·æ±‚ä¹‹é—´ç­‰å¾…
                    nonlocal last_llm_time
                    if llm_req_interval > 0.0 and last_llm_time > 0.0:
                        now = _pytime.time()
                        wait = (last_llm_time + llm_req_interval) - now
                        if wait > 0:
                            if debug:
                                logc(f"[LLM] throttle: sleep {wait:.2f}s", ANSI_CYAN)
                            _pytime.sleep(wait)
                    llm_attempted = True
                    _t0 = _pytime.time()
                    txt = call_llm(llm_prompt, model=gemini_model)
                    _dt = _pytime.time() - _t0
                    last_llm_time = _pytime.time()
                    llm_raw = str(txt) if txt is not None else None
                    _is_err = isinstance(txt, str) and (
                        txt.startswith("[Gemini Error]") or txt.startswith("[Gemini HTTPError]")
                    )
                    if _is_err:
                        llm_used = False
                        llm_status = "skipped_exception"
                        # LLM å¤±è´¥ï¼šä¸çº³å…¥è®­ç»ƒ
                        train_include = False
                        train_reason = "llm_failed"
                        # å›é€€ä½¿ç”¨å¯å‘å¼ä½œä¸ºæ ‡ç­¾å ä½
                        label = int(bool(heur_ok))
                        # è‹¥ä¸º 429 (RESOURCE_EXHAUSTED)ï¼Œè§£æå»ºè®®çš„ Retry-After å¹¶ä¼‘çœ ä»¥ç¼“è§£é…é¢é™åˆ¶
                        try:
                            import re as _re
                            s = llm_raw or ""
                            # æ–¹å¼1ï¼šä»é”™è¯¯æç¤ºæ–‡æœ¬ä¸­æå– "Please retry in Xs."
                            m1 = _re.search(r"Please retry in\s+([0-9.]+)s", s)
                            # æ–¹å¼2ï¼šä» JSON å­—æ®µæå– \"retryDelay\": \"Xs\"
                            m2 = _re.search(r"\"retryDelay\"\s*:\s*\"([0-9.]+)s\"", s)
                            sec = None
                            if m1:
                                sec = float(m1.group(1))
                            elif m2:
                                sec = float(m2.group(1))
                            if sec and sec > 0:
                                if debug:
                                    logc(f"[LLM] 429 quota: retry-after {sec:.2f}s -> sleeping", ANSI_CYAN)
                                _pytime.sleep(sec)
                                # æ›´æ–°èŠ‚æµæ—¶é—´åŸºå‡†ï¼Œé¿å…ç´§æ¥ç€å†æ¬¡è§¦å‘
                                last_llm_time = _pytime.time()
                        except Exception:
                            pass
                    else:
                        llm_used = True
                        llm_status = "used"
                        if isinstance(txt, str):
                            _s = txt.strip()
                            if _s == "1" or ("1" in _s and "0" not in _s):
                                label = 1
                                train_include = True
                                train_reason = "llm_1"
                            elif _s == "0" or ("0" in _s and "1" not in _s):
                                label = 0
                                train_include = True
                                train_reason = "llm_0"
                            else:
                                # æ— æ³•è§£æï¼šå½“ä½œå¤±è´¥ï¼Œä¸çº³å…¥
                                train_include = False
                                train_reason = "llm_unparsed"
                                # å›é€€ä½¿ç”¨å¯å‘å¼ä½œä¸ºæ ‡ç­¾å ä½
                                label = int(bool(heur_ok))
                    if debug:
                        # è¿”å›ç»“æœæ‰“å°
                        try:
                            _rprev = llm_raw if isinstance(llm_raw, str) else "<None>"
                            _rprev2 = _rprev[:240] + (
                                "..." if isinstance(_rprev, str) and len(_rprev) > 240 else ""
                            )
                            _msg2 = (
                                f"[LLM] done: used={llm_used}, status={llm_status}, dt={_dt:.2f}s, "
                                f"result_len={len(llm_raw) if isinstance(llm_raw, str) else 0}, result_preview={_rprev2}"
                            )
                            logc(_msg2, ANSI_CYAN)
                            # ç»“æœé«˜äº®ï¼ˆå«ä¸­æ–‡è§£é‡Šï¼‰
                            if _is_err:
                                logc("[RESULT] LLM è¯·æ±‚å¤±è´¥/æœªè§£æ â†’ ä¸çº³å…¥è®­ç»ƒ", ANSI_RED)
                            else:
                                try:
                                    _s = (llm_raw or "").strip()
                                except Exception:
                                    _s = ""
                                if _s == "1" or ("1" in _s and "0" not in _s):
                                    logc("[RESULT] 1 â†’ ç¬¦åˆå…¬ç†ç³»ç»Ÿï¼ˆæ­£æ ·æœ¬ï¼Œçº³å…¥è®­ç»ƒï¼‰", ANSI_GREEN)
                                elif _s == "0" or ("0" in _s and "1" not in _s):
                                    logc("[RESULT] 0 â†’ ä¸ç¬¦åˆå…¬ç†ç³»ç»Ÿï¼ˆè´Ÿæ ·æœ¬ï¼Œçº³å…¥è®­ç»ƒï¼‰", ANSI_RED)
                                else:
                                    logc("[RESULT] æœªèƒ½è§£æä¸º 1/0 â†’ ä¸çº³å…¥è®­ç»ƒ", ANSI_MAGENTA)
                            logc("========== LLM REQUEST END ==========", ANSI_CYAN)
                        except Exception:
                            pass
                except Exception as e:
                    llm_attempted = True
                    llm_used = False
                    llm_status = "skipped_exception"
                    llm_raw = None
                    # LLM å¼‚å¸¸ï¼šä¸çº³å…¥è®­ç»ƒ
                    train_include = False
                    train_reason = "llm_exception"
                    if debug:
                        try:
                            _msgE = f"[LLM] exception: {e}"
                            logc(_msgE, ANSI_RED)
                            logc("[RESULT] LLM å¼‚å¸¸ â†’ ä¸çº³å…¥è®­ç»ƒ", ANSI_RED)
                            logc("========== LLM REQUEST END ==========", ANSI_CYAN)
                        except Exception:
                            pass
            elif warns_present and not bool(cfg.get("use_llm_oracle", False)):
                # æœªå¯ç”¨ LLMï¼šä¸çº³å…¥è®­ç»ƒï¼›æ ‡ç­¾å›é€€ä¸ºå¯å‘å¼
                llm_status = "skipped_use_llm_false"
                train_include = False
                train_reason = "warnings_unresolved"
                label = int(bool(heur_ok))
            else:
                # æ— é”™è¯¯æ— è­¦å‘Šï¼šæ˜ç¡®æ­£ç¡®
                llm_status = "skipped_no_warn"
                label = 1
                train_include = True
                train_reason = "syntax_ok"
        # æ§åˆ¶å°è°ƒè¯•è¾“å‡º
        if debug:
            print(
                "[SAMPLE] seq={seq} label={label} syntax_errors={e} warnings={w} "
                "heur_ok={heur} llm_attempted={la} llm_used={lu} llm_status={ls} "
                "train_include={ti} train_reason={tr}".format(
                    seq=seq,
                    label=label,
                    e=len(syntax["errors"]),
                    w=len(syntax["warnings"]),
                    heur=heur_ok,
                    la=llm_attempted,
                    lu=llm_used,
                    ls=llm_status,
                    ti=train_include,
                    tr=train_reason,
                )
            )
        return {
            "label": int(label),
            "syntax": syntax,
            "heur_ok": bool(heur_ok),
            "delta_risk": float(dr),
            "cost": float(c),
            "llm_used": bool(llm_used),
            "llm_raw": llm_raw,
            "llm_prompt": llm_prompt,
            "llm_status": llm_status,
            "llm_attempted": bool(llm_attempted),
            "train_include": bool(train_include),
            "train_reason": train_reason,
        }

    X_rows: list[list[float]] = []
    y_rows: list[float] = []
    for i in range(n_samples):
        seq = sample_random_package(spec, min_len=min_len, max_len=max_len, no_consecutive_duplicate=no_dup)
        # ç‰¹å¾ï¼šop è®¡æ•°
        cnts = [float(seq.count(nm)) for nm in op_names]
        # ä½œç”¨æ•ˆæœç‰¹å¾
        _, dr, c = apply_sequence(this_domain, init_state, seq)
        length = float(len(seq))
        vec = cnts + [length, float(dr), float(c)]
        judge = evaluate_with_details(this_domain, seq, init_state)
        label_i = int(judge["label"])
        if bool(judge.get("train_include", False)):
            X_rows.append([float(x) for x in vec])
            y_rows.append(float(label_i))
        if debug_dump:
            # å‚æ•°åŒ–åŠ¨ä½œï¼ˆv1 ç©ºé—´ä¸­ä½å€¼ï¼‰
            try:
                from lbopb.src.rlsac.kernel.rlsac_pathfinder.op_space_utils import load_op_space, param_grid_of, params_from_grid  # type: ignore
                _mod_dir = Path(__file__).resolve().parent
                _space_ref = _mod_dir / "operator_spaces" / f"{this_domain}_op_space.v1.json"
                ops_det = None
                os_meta = None
                if _space_ref.exists():
                    space = load_op_space(str(_space_ref))
                    steps = []
                    for nm in list(seq):
                        try:
                            names, grids = param_grid_of(space, nm)
                        except Exception:
                            steps.append({"name": nm})
                            continue
                        gi = []
                        for g in grids:
                            L = len(g)
                            gi.append(max(0, (L - 1) // 2))
                        prs = params_from_grid(space, nm, gi)
                        steps.append({"name": nm, "grid_index": gi, "params": prs})
                    ops_det = steps
                    os_meta = {
                        "op_space_id": space.get("space_id", f"{this_domain}.v1"),
                        "op_space_ref": str(_space_ref.relative_to(Path(__file__).resolve().parents[5])).replace("\\", "/"),
                    }
            except Exception:
                ops_det = None
                os_meta = None
            rec = {
                "index": int(i),
                "sequence": list(seq),
                "features": {
                    "bag_of_ops": {op_names[j]: float(cnts[j]) for j in range(len(op_names))},
                    "length": float(length),
                    "delta_risk": float(dr),
                    "cost": float(c)
                },
                "judge": judge,
                "label": int(label_i),
                "train_include": bool(judge.get("train_include", False)),
                "train_reason": str(judge.get("train_reason", ""))
            }
            if ops_det:
                rec["ops_detailed"] = ops_det
            if os_meta:
                rec.update(os_meta)
            dataset_dump.append(rec)
    # æ„é€ è®­ç»ƒå¼ é‡
    if len(X_rows) > 0:
        X_t = torch.tensor(X_rows, dtype=torch.float32)
        y_t = torch.tensor(y_rows, dtype=torch.float32)
    else:
        X_t = torch.zeros((0, feat_dim), dtype=torch.float32)
        y_t = torch.zeros((0,), dtype=torch.float32)

    # åœ¨ explore_only æ¨¡å¼ä¸‹ï¼Œä¸è¿›è¡Œä»»ä½•æ¨¡å‹è®­ç»ƒï¼ˆä»…ç”Ÿæˆæ¢ç´¢æ ·æœ¬ï¼‰
    model = None
    if not explore_only:
        model = PackageScorer(feat_dim).to(device)
        if X_t.shape[0] > 0:
            train_scorer(model, X_t.to(device), y_t.to(device), epochs=epochs, batch_size=batch_size,
                         lr=float(cfg.get("learning_rate_actor", 3e-4)))

    # æ ¡éªŒå¹¶å¯é€‰è¿­ä»£ç›´è‡³å®Œç¾åŒ¹é…
    fit_until_perfect = bool(cfg.get("fit_until_perfect", True))
    max_refit_rounds = int(cfg.get("max_refit_rounds", 10))
    refit_epochs = int(cfg.get("refit_epochs", epochs))
    rounds = 0
    if not explore_only and X_t.shape[0] > 0 and model is not None:
        with torch.no_grad():
            preds = (model(X_t.to(device)).cpu().view(-1) >= 0.5).to(torch.float32)
            acc = float((preds == y_t).to(torch.float32).mean().item())
        while fit_until_perfect and acc < 1.0 and rounds < max_refit_rounds:
            train_scorer(model, X_t.to(device), y_t.to(device), epochs=refit_epochs, batch_size=batch_size,
                         lr=float(cfg.get("learning_rate_actor", 3e-4)))
            with torch.no_grad():
                preds = (model(X_t.to(device)).cpu().view(-1) >= 0.5).to(torch.float32)
                acc = float((preds == y_t).to(torch.float32).mean().item())
            rounds += 1
    else:
        acc = 1.0

    # è®­ç»ƒå®Œç”Ÿæˆå¤§é‡å€™é€‰ï¼Œæ‰“åˆ†é€‰ Top-Kï¼Œå†™å…¥è¾æµ·
    cand_n = int(cfg.get("candidate_generate", 1000))
    scored: list[tuple[float, List[str], float, float]] = []
    if explore_only:
        # æ¢ç´¢æ¨¡å¼ä¸‹ä¸ä¾èµ–æ¨¡å‹ï¼Œä»¥å¯å‘å¼åˆ†æ•°æ’åºï¼ˆdelta_risk - lambda*costï¼‰
        for _ in range(cand_n):
            seq = sample_random_package(spec, min_len=min_len, max_len=max_len, no_consecutive_duplicate=no_dup)
            _, dr, c = apply_sequence(this_domain, init_state, seq)
            score = float(dr) - cost_lambda * float(c)
            scored.append((score, seq, float(dr), float(c)))
    else:
        with torch.no_grad():
            for _ in range(cand_n):
                seq = sample_random_package(spec, min_len=min_len, max_len=max_len, no_consecutive_duplicate=no_dup)
                cnts = [float(seq.count(nm)) for nm in op_names]
                _, dr, c = apply_sequence(this_domain, init_state, seq)
                length = float(len(seq))
                if model is None:
                    # ç†è®ºä¸Š explore_only=False æ—¶æ‰ä¼šèµ°åˆ°è¿™é‡Œï¼›å…œåº•é˜²å¾¡
                    score = float(dr) - cost_lambda * float(c)
                else:
                    vec = torch.tensor([*(cnts), length, float(dr), float(c)], dtype=torch.float32).unsqueeze(0).to(device)
                    score = float(model(vec).item())
                scored.append((score, seq, float(dr), float(c)))
    scored.sort(key=lambda t: t[0], reverse=True)
    if debug_dump:
        try:
            # ç»Ÿè®¡ warnings / llm è¦†ç›–ç‡ä¸è®­ç»ƒçº³å…¥æƒ…å†µ
            total = len(dataset_dump)
            err_cnt = 0;
            warn_cnt = 0;
            llm_cnt = 0;
            syntax_ok_cnt = 0;
            llm_attempted_cnt = 0
            llm_used_cnt = 0;
            llm_skip_no_warn = 0;
            llm_skip_use_llm_false = 0;
            llm_skip_errors = 0;
            llm_skip_exception = 0
            train_included_cnt = 0
            train_reason_cnt: Dict[str, int] = {}
            for item in dataset_dump:
                j = item.get("judge", {})
                syn = j.get("syntax", {}) if isinstance(j, dict) else {}
                errors = syn.get("errors", []) or []
                warns = syn.get("warnings", []) or []
                if len(errors) > 0:
                    err_cnt += 1
                if len(warns) > 0:
                    warn_cnt += 1
                if len(errors) == 0 and len(warns) == 0:
                    syntax_ok_cnt += 1
                if bool(j.get("llm_used", False)):
                    llm_cnt += 1
                if bool(j.get("llm_attempted", False)):
                    llm_attempted_cnt += 1
                # ç»†åˆ† llm çŠ¶æ€
                ls = str(j.get("llm_status", ""))
                if ls == "used":
                    llm_used_cnt += 1
                elif ls == "skipped_no_warn":
                    llm_skip_no_warn += 1
                elif ls == "skipped_use_llm_false":
                    llm_skip_use_llm_false += 1
                elif ls == "skipped_errors":
                    llm_skip_errors += 1
                elif ls == "skipped_exception":
                    llm_skip_exception += 1
                # è®­ç»ƒçº³å…¥ç»Ÿè®¡
                if bool(item.get("train_include", False)):
                    train_included_cnt += 1
                tr = str(item.get("train_reason", ""))
                if tr:
                    train_reason_cnt[tr] = int(train_reason_cnt.get(tr, 0)) + 1
            # æ§åˆ¶å°ä¸æ—¥å¿—æ‰“å°
            msg_stats = (
                f"[STATS] total={total} error_samples={err_cnt} warning_samples={warn_cnt} "
                f"syntax_ok_samples={syntax_ok_cnt} llm_attempted_samples={llm_attempted_cnt} llm_used_samples={llm_cnt} "
                f"train_included_samples={train_included_cnt} train_excluded_samples={total - train_included_cnt}"
            )
            print(msg_stats)
            if log_to_file and 'logger' in locals() and logger:
                logger.write_line(msg_stats)
            # æ‰“å° LLM ç»†åˆ†ç»Ÿè®¡
            msg_llm = (
                f"[LLM_STATS] attempted={llm_attempted_cnt} used={llm_used_cnt} skipped_no_warn={llm_skip_no_warn} "
                f"skipped_use_llm_false={llm_skip_use_llm_false} skipped_errors={llm_skip_errors} "
                f"skipped_exception={llm_skip_exception}"
            )
            print(msg_llm)
            if log_to_file and 'logger' in locals() and logger:
                logger.write_line(msg_llm)
            # æ‰“å°è®­ç»ƒçº³å…¥åŸå› ç»Ÿè®¡
            try:
                _reasons = ", ".join([f"{k}={v}" for k, v in train_reason_cnt.items()])
            except Exception:
                _reasons = ""
            msg_train = f"[TRAIN_STATS] included={train_included_cnt} reasons: {_reasons}"
            print(msg_train)
            if log_to_file and 'logger' in locals() and logger:
                logger.write_line(msg_train)

            debug_ds_path = run_dir / "debug_dataset.json"
            debug_cand_path = run_dir / "debug_candidates.json"
            with open(debug_ds_path, 'w', encoding='utf-8', newline='') as f:
                json.dump({
                    "domain": this_domain,
                    "op_names": op_names,
                    "samples": dataset_dump,
                    "fit_rounds": rounds if fit_until_perfect else 0,
                    "train_acc": acc,
                    "stats": {
                        "total": total,
                        "error_samples": err_cnt,
                        "warning_samples": warn_cnt,
                        "syntax_ok_samples": syntax_ok_cnt,
                        "llm_attempted_samples": llm_attempted_cnt,
                        "llm_used_samples": llm_cnt,
                        "train": {
                            "included": train_included_cnt,
                            "excluded": total - train_included_cnt,
                            "reasons": train_reason_cnt
                        },
                        "llm_detail": {
                            "used": llm_used_cnt,
                            "skipped_no_warn": llm_skip_no_warn,
                            "skipped_use_llm_false": llm_skip_use_llm_false,
                            "skipped_errors": llm_skip_errors,
                            "skipped_exception": llm_skip_exception
                        }
                    }
                }, f, ensure_ascii=False, indent=2)
            with open(debug_cand_path, 'w', encoding='utf-8', newline='') as f:
                json.dump([
                    {"score": float(s), "sequence": seq, "delta_risk": float(dr), "cost": float(c)}
                    for (s, seq, dr, c) in scored[: int(cfg.get("debug_candidates_top", 50))]
                ], f, ensure_ascii=False, indent=2)
            # ç”Ÿæˆå¸¦æ ‡ç­¾çš„ç®—å­åŒ…æ–‡ä»¶ï¼ˆä¾›è®­ç»ƒ/å¤æ ¸ä½¿ç”¨ï¼‰
            try:
                import hashlib as _hashlib
                labeled_path = run_dir / f"{this_domain}_operator_packages_labeled.json"
                pkgs_map: dict[str, dict] = {}
                now_ts = int(_pytime.time())
                for srec in dataset_dump:
                    seqv = list(srec.get("sequence", []) or [])
                    feats = srec.get("features", {}) or {}
                    dr_v = float(feats.get("delta_risk", 0.0))
                    c_v = float(feats.get("cost", 0.0))
                    len_v = int(feats.get("length", len(seqv)))
                    sc_v = dr_v - cost_lambda * c_v
                    steps = srec.get("ops_detailed", None)
                    meta = {k: srec.get(k) for k in ("op_space_id", "op_space_ref") if k in srec}
                    # æ±‡æ€»æ ¡éªŒï¼ˆsyntax_checker æˆ– syntax+Geminiï¼‰çš„ç»“æœä¸ºä¸­æ–‡æè¿°
                    j = srec.get("judge", {}) or {}
                    syn = j.get("syntax", {}) or {}
                    syn_errs = list(syn.get("errors", []) or [])
                    syn_warns = list(syn.get("warnings", []) or [])
                    if syn_errs:
                        _syn_res_cn = "é”™è¯¯"
                    elif syn_warns:
                        _syn_res_cn = "è­¦å‘Š"
                    else:
                        _syn_res_cn = "æ­£ç¡®"
                    _llm_attempted = bool(j.get("llm_attempted", False))
                    _llm_used = bool(j.get("llm_used", False)) and str(j.get("llm_status", "")) == "used"
                    _gem_res_cn = None
                    try:
                        _raw = j.get("llm_raw", None)
                        if _llm_used and isinstance(_raw, str):
                            _s = _raw.strip()
                            if (_s == "1") or ("1" in _s and "0" not in _s):
                                _gem_res_cn = "æ­£ç¡®"
                            elif (_s == "0") or ("0" in _s and "1" not in _s):
                                _gem_res_cn = "é”™è¯¯"
                    except Exception:
                        _gem_res_cn = None
                    _validation = {
                        "mode": "dual" if _llm_attempted else "syntax_only",
                        "syntax": {
                            "result": _syn_res_cn,
                            "errors": len(syn_errs),
                            "warnings": len(syn_warns),
                        },
                        "gemini": ({
                            "used": True,
                            **({"result": _gem_res_cn} if _gem_res_cn is not None else {}),
                        } if _llm_attempted else {"used": False}),
                    }
                    # ä½¿ç”¨ä¸èšåˆå™¨ä¸€è‡´çš„å»é‡é”®ï¼šdomain + sequence + ops_detailed
                    payload = {"domain": this_domain, "sequence": seqv, "ops_detailed": steps or []}
                    blob = json.dumps(payload, ensure_ascii=False, sort_keys=True, separators=(",", ":")).encode("utf-8")
                    hid = int(_hashlib.sha1(blob).hexdigest(), 16) % (10**10)
                    sid = f"pkg_{this_domain}_{hid}"
                    pkg = {
                        "id": sid,
                        "domain": this_domain,
                        "sequence": seqv,
                        "length": len_v,
                        "delta_risk": dr_v,
                        "cost": c_v,
                        "score": sc_v,
                        "created_at": now_ts,
                        "updated_at": now_ts,
                        "source": "training_dataset",
                        "label": int(srec.get("label", 0)),
                    }
                    # å†™å…¥æ ¡éªŒæ‘˜è¦
                    pkg["validation"] = _validation
                    if steps:
                        pkg["ops_detailed"] = steps
                    if meta:
                        pkg.update(meta)
                    prev = pkgs_map.get(sid)
                    if (prev is None) or (float(sc_v) > float(prev.get("score", -1e9))):
                        pkgs_map[sid] = pkg
                labeled_items = list(pkgs_map.values())
                labeled_items.sort(key=lambda d: (-float(d.get("score", 0.0)), int(d.get("length", 0)), tuple(str(x) for x in d.get("sequence", []))))
                with labeled_path.open('w', encoding='utf-8', newline='\n') as f:
                    f.write(json.dumps(labeled_items, ensure_ascii=False, indent=2))
                if debug:
                    print(f"[TRAIN] labeled packages written: {labeled_path} items={len(labeled_items)}")
            except Exception:
                pass
            # å°†æ­£ç¡®æ ·æœ¬ï¼ˆlabel=1ï¼‰çš„ç®—å­åŒ…å»é‡çº³å…¥ä¸“ç”¨ç›®å½•ï¼Œå¹¶æŒ‰ score é‡æ–°æ’åº
            try:
                try:
                    from .package_store import ingest_from_debug_dataset  # type: ignore
                except Exception:
                    from lbopb.src.rlsac.kernel.rlsac_pathfinder.package_store import \
                        ingest_from_debug_dataset  # type: ignore
                ingest_from_debug_dataset(debug_ds_path, domain=this_domain, cost_lambda=cost_lambda)
            except Exception:
                pass
        except Exception:
            pass

    # é€‰æ‹© Top-K å¹¶å†™å…¥æœ¬åŸŸè¾æµ·ï¼ˆrun_dir ä¸‹ä¸å…¨å±€ï¼‰
    unique = []
    seen = set()
    for score, seq, dr, c in scored:
        t = tuple(seq)
        if t in seen:
            continue
        seen.add(t)
        unique.append((score, seq, dr, c))
        if len(unique) >= topk:
            break

    # ä¿å­˜æ¨¡å‹æƒé‡ï¼ˆå¯é€‰ï¼‰
    if not explore_only:
        torch.save(model.state_dict(), run_dir / "scorer.pt")

    # è®­ç»ƒç»“æŸåï¼Œè‡ªåŠ¨æå–ç®—å­åŒ…å¹¶å†™å…¥è®­ç»ƒç›®å½•
    if not explore_only:
        try:
            pkg = extract_operator_package(run_dir, cfg_path, domain_override=this_domain)
            run_dict_path = run_dir / f"{this_domain}_operator_packages.json"
            arr: List[Dict[str, Any]] = []
            if run_dict_path.exists():
                try:
                    arr = json.loads(run_dict_path.read_text(encoding="utf-8"))
                except Exception:
                    arr = []
            arr.append(pkg)
            text = json.dumps(arr, ensure_ascii=False, indent=2)
            text = text.replace("\r\n", "\n")
            with run_dict_path.open("w", encoding="utf-8", newline="\n") as f:
                f.write(text)
        except Exception:
            pass

    print(f"Training finished. Artifacts at: {run_dir}")
    return run_dir


def train_all(config_path: str | Path | None = None, domains: list[str] | None = None) -> list[Path]:
    """æŒ‰é¡ºåºè®­ç»ƒå¤šä¸ªåŸŸï¼ˆé»˜è®¤ä¸ƒåŸŸï¼‰ï¼Œå¹¶åˆ†åˆ«æå–ç®—å­åŒ…ã€‚

    è¿”å›å„åŸŸçš„è®­ç»ƒè¾“å‡ºç›®å½•åˆ—è¡¨ã€‚
    """
    doms = domains or ["pem", "pdem", "pktm", "pgom", "tem", "prm", "iem"]
    out: list[Path] = []
    for d in doms:
        try:
            rd = train(config_path, domain_override=d)
            out.append(rd)
        except Exception:
            continue
    return out


def extract_operator_package(run_dir: str | Path, config_path: str | Path | None = None, max_len: int | None = None) -> \
        Dict[str, Any]:
    """è´ªå¿ƒè§£ç æå–ç®—å­åŒ…ï¼Œå†™å…¥å¯¹åº”åŸŸçš„ operator_packages.jsonã€‚"""
    mod_dir = Path(__file__).resolve().parent
    cfg_path = Path(config_path) if config_path else (mod_dir / "config.json")
    cfg = _load_config(cfg_path)
    domain = _resolve_domain(cfg, None)
    env = build_env_from_config(cfg_path, domain_override=domain)
    obs_dim = env.observation_space.shape[0]
    n_actions = env.action_space.high - env.action_space.low

    # åŠ è½½ç­–ç•¥
    run_dir = Path(run_dir)
    global DiscretePolicy
    if DiscretePolicy is None:
        import torch.nn as nn
        import torch.nn.functional as F  # noqa: F401
        class _FallbackDiscretePolicy(nn.Module):
            def __init__(self, od: int, na: int, hidden=(128, 128)):
                super().__init__()
                layers = []
                last = od
                for h in hidden:
                    layers.append(nn.Linear(last, h))
                    layers.append(nn.ReLU())
                    last = h
                layers.append(nn.Linear(last, na))
                self.net = nn.Sequential(*layers)

            def forward(self, x: torch.Tensor):
                logits = self.net(x)
                probs = torch.softmax(logits, dim=-1)
                return logits, probs

        DiscretePolicy = _FallbackDiscretePolicy  # type: ignore
    policy = DiscretePolicy(obs_dim, n_actions)
    policy.load_state_dict(torch.load(run_dir / "policy.pt", map_location="cpu"))
    policy.eval()

    # è´ªå¿ƒ rollout
    names = {v: k for k, v in env.op2idx.items()}
    seq: List[str] = []
    s = env.reset().unsqueeze(0)
    steps = 0
    max_len = int(max_len or env._max_steps)
    with torch.no_grad():
        while steps < max_len:
            logits, probs = policy(s)
            a = int(torch.argmax(probs, dim=-1).item())
            seq.append(names.get(a, str(a)))
            s2, r, d, _ = env.step(a)
            s = s2.unsqueeze(0)
            steps += 1
            if d:
                break

    # å†™å…¥ package å­—å…¸
    pkg = {
        "id": f"pkg_{domain}_{int(_pytime.time())}",
        "domain": domain,
        "initial": {
            "b": env._init_state_cfg.b,
            "n_comp": env._init_state_cfg.n_comp,
            "perim": env._init_state_cfg.perim,
            "fidelity": env._init_state_cfg.fidelity,
        },
        "target": {
            "b": env._goal.target.b,
            "n_comp": env._goal.target.n_comp,
            "perim": env._goal.target.perim,
            "fidelity": env._goal.target.fidelity,
        },
        "sequence": seq,
        "max_steps": max_len,
        "created_at": int(_pytime.time()),
    }

    dict_path = mod_dir / get_domain_spec(domain).dict_filename
    arr: List[Dict[str, Any]] = []
    if dict_path.exists():
        try:
            arr = json.loads(dict_path.read_text(encoding="utf-8"))
        except Exception:
            arr = []
    arr.append(pkg)
    text = json.dumps(arr, ensure_ascii=False, indent=2)
    text = text.replace("\r\n", "\n")
    with dict_path.open("w", encoding="utf-8", newline="\n") as f:
        f.write(text)
    return pkg


if __name__ == "__main__":
    out = train()
    print(out)
