# SPDX-License-Identifier: GPL-3.0-only
# Copyright (C) 2025 GaoZheng
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, version 3 only.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
#
# --- è‘—ä½œæƒç‹¬ç«‹æ€§å£°æ˜Ž (Copyright Independence Declaration) ---
# æœ¬æ–‡ä»¶ï¼ˆâ€œè½½è·â€ï¼‰æ˜¯ä½œè€… (GaoZheng) çš„åŽŸåˆ›è‘—ä½œç‰©ï¼Œå…¶çŸ¥è¯†äº§æƒ
# ç‹¬ç«‹äºŽå…¶è¿è¡Œå¹³å° GROMACSï¼ˆâ€œå®¿ä¸»â€ï¼‰ã€‚
# æœ¬æ–‡ä»¶çš„æŽˆæƒéµå¾ªä¸Šè¿° SPDX æ ‡è¯†ï¼Œä¸å—â€œå®¿ä¸»â€è®¸å¯è¯çš„ç®¡è¾–ã€‚
# è¯¦æƒ…å‚è§é¡¹ç›®æ–‡æ¡£ "my_docs/project_docs/1762636780_ðŸš©ðŸš©gromacs-2024.1_developeré¡¹ç›®çš„è‘—ä½œæƒè®¾è®¡ç­–ç•¥ï¼šâ€œå®¿ä¸»-è½½è·â€ä¸Žâ€œåŒè½¨åˆ¶â€å¤åˆæž¶æž„.md"ã€‚
# ------------------------------------------------------------------

from __future__ import annotations

from typing import List, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F


class PackageScorer(nn.Module):
    """å¯¹ç®—å­åŒ…ï¼ˆç‰¹å¾å‘é‡ï¼‰è¿›è¡Œè¯„åˆ†ï¼ˆ0..1ï¼‰ã€‚"""

    def __init__(self, in_dim: int, hidden: Tuple[int, int] = (128, 64)) -> None:
        super().__init__()
        h1, h2 = hidden
        self.net = nn.Sequential(
            nn.Linear(in_dim, h1), nn.ReLU(),
            nn.Linear(h1, h2), nn.ReLU(),
            nn.Linear(h2, 1)
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return torch.sigmoid(self.net(x)).view(-1)


def train_scorer(
        model: PackageScorer,
        train_x: torch.Tensor,
        train_y: torch.Tensor,
        *,
        epochs: int = 20,
        batch_size: int = 64,
        lr: float = 3e-4,
) -> None:
    opt = torch.optim.Adam(model.parameters(), lr=lr)
    n = train_x.shape[0]
    for ep in range(epochs):
        idx = torch.randperm(n)
        x = train_x[idx]
        y = train_y[idx]
        for i in range(0, n, batch_size):
            xb = x[i:i + batch_size]
            yb = y[i:i + batch_size]
            pred = model(xb)
            loss = F.binary_cross_entropy(pred, yb)
            opt.zero_grad();
            loss.backward();
            opt.step()
