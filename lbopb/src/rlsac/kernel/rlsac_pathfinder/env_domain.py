# SPDX-License-Identifier: GPL-3.0-only
# Copyright (C) 2025 GaoZheng
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, version 3 only.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <https://www.gnu.org/licenses/>.
#
# --- è‘—ä½œæƒç‹¬ç«‹æ€§å£°æ˜Ž (Copyright Independence Declaration) ---
# æœ¬æ–‡ä»¶ï¼ˆâ€œè½½è·â€ï¼‰æ˜¯ä½œè€… (GaoZheng) çš„åŽŸåˆ›è‘—ä½œç‰©ï¼Œå…¶çŸ¥è¯†äº§æƒ
# ç‹¬ç«‹äºŽå…¶è¿è¡Œå¹³å° GROMACSï¼ˆâ€œå®¿ä¸»â€ï¼‰ã€‚
# æœ¬æ–‡ä»¶çš„æŽˆæƒéµå¾ªä¸Šè¿° SPDX æ ‡è¯†ï¼Œä¸å—â€œå®¿ä¸»â€è®¸å¯è¯çš„ç®¡è¾–ã€‚
# è¯¦æƒ…å‚è§é¡¹ç›®æ–‡æ¡£ "my_docs/project_docs/1762636780_ðŸš©ðŸš©gromacs-2024.1_developeré¡¹ç›®çš„è‘—ä½œæƒè®¾è®¡ç­–ç•¥ï¼šâ€œå®¿ä¸»-è½½è·â€ä¸Žâ€œåŒè½¨åˆ¶â€å¤åˆæž¶æž„.md"ã€‚
# ------------------------------------------------------------------

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Tuple

import torch

from .domain import DomainSpec
from lbopb.src.rlsac.application.rlsac_nsclc.space import SimpleBoxFloat32, SimpleBoxInt32


@dataclass
class Goal:
    target: Any  # ç›®æ ‡çŠ¶æ€ï¼ˆåŸŸçŠ¶æ€ç±»åž‹ï¼‰
    tol_b: float = 1e-3
    tol_n: float = 0.5
    tol_p: float = 1e-3
    tol_f: float = 1e-3


class DomainPathfinderEnv:
    """ç»Ÿä¸€çš„å•åŸŸè·¯å¾„æŽ¢ç´¢çŽ¯å¢ƒï¼ˆæ”¯æŒ pem/pdem/pktm/pgom/tem/prm/iemï¼‰ã€‚

    - Observation: [b, n_comp, perim, fidelity]ï¼ˆfloat32, shape=(4,)ï¼‰
    - Action: ç¦»æ•£åŸºæœ¬ç®—å­é›†çš„ç´¢å¼•ï¼ˆint32, range [0, N)ï¼‰
    - Done: è¾¾åˆ°ç›®æ ‡å®¹å·®å†…ï¼Œæˆ–æ­¥æ•°ä¸Šé™
    - Reward: è·ç¦»æ”¹è¿› + ç›®æ ‡å¥–åŠ± - æ­¥é•¿æƒ©ç½š
    """

    def __init__(
            self,
            spec: DomainSpec,
            *,
            init_state: Any,
            goal: Goal,
            max_steps: int = 64,
            improve_weight: float = 1.0,
            step_penalty: float = 0.01,
            include_identity: bool = False,
    ) -> None:
        self._spec = spec
        self._init_state_cfg = init_state
        self._goal = goal
        self._max_steps = int(max_steps)
        self._improve_w = float(improve_weight)
        self._step_penalty = float(step_penalty)
        self._steps = 0
        self._state = init_state

        self._ops: List[Any] = []
        if include_identity:
            self._ops.append(spec.identity_cls())
        for cls in spec.op_classes:
            self._ops.append(cls())
        self._op2idx: Dict[str, int] = {self._op_name(o): i for i, o in enumerate(self._ops)}

        self.observation_space = SimpleBoxFloat32(low=0.0, high=10.0, shape=(4,))
        self.action_space = SimpleBoxInt32(low=0, high=len(self._ops), shape=(1,))

    @staticmethod
    def _op_name(o: Any) -> str:
        return getattr(o, "name", o.__class__.__name__)

    @property
    def op2idx(self) -> Dict[str, int]:
        return dict(self._op2idx)

    def reset(self) -> torch.Tensor:
        self._state = self._init_state_cfg.clamp()
        self._steps = 0
        return self._to_obs(self._state)

    def _to_obs(self, s: Any) -> torch.Tensor:
        vec = torch.tensor([float(s.b), float(s.n_comp), float(s.perim), float(s.fidelity)], dtype=torch.float32)
        return vec

    def _distance(self, s: Any) -> float:
        t = self._goal.target
        db = abs(float(s.b) - float(t.b))
        dn = abs(float(s.n_comp) - float(t.n_comp))
        dp = abs(float(s.perim) - float(t.perim))
        df = abs(float(s.fidelity) - float(t.fidelity))
        # ç®€å•åŠ æƒ L1 è·ç¦»ï¼›å¯æŒ‰åŸŸè‡ªå®šä¹‰
        return (1.0 * db) + (0.3 * dn) + (0.5 * dp) + (1.0 * df)

    def _is_goal(self, s: Any) -> bool:
        t = self._goal.target
        return (
                abs(float(s.b) - float(t.b)) <= self._goal.tol_b and
                abs(float(s.n_comp) - float(t.n_comp)) <= self._goal.tol_n and
                abs(float(s.perim) - float(t.perim)) <= self._goal.tol_p and
                abs(float(s.fidelity) - float(t.fidelity)) <= self._goal.tol_f
        )

    def step(self, action: torch.Tensor | int) -> Tuple[torch.Tensor, float, bool, Dict[str, float]]:
        a = int(action if isinstance(action, int) else int(action.item()))
        a = max(0, min(a, len(self._ops) - 1))
        prev = self._state
        prev_d = self._distance(prev)
        self._steps += 1

        op = self._ops[a]
        cur = op(prev).clamp()
        self._state = cur
        cur_d = self._distance(cur)

        improve = (prev_d - cur_d)
        done = self._is_goal(cur) or (self._steps >= self._max_steps)
        reward = self._improve_w * improve - self._step_penalty
        if self._is_goal(cur):
            reward += 5.0

        return self._to_obs(cur), float(reward), bool(done), {"improve": float(improve), "dist": float(cur_d)}
